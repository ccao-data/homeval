---
title: "pin-val-analytics"
format: html
---

```{r}
library(arrow)
library(DBI)
library(dplyr)
library(fs)
library(glue)
library(lubridate)
library(paws)
library(purrr)
library(tibble)
library(tidyverse)
```


```{r}
AWS_ATHENA_CONN_NOCTUA <- dbConnect(noctua::athena())

cache_dir <- "data/cache"
dir.create(cache_dir, showWarnings = FALSE)

cache_file <- file.path(cache_dir, "assessment_card.parquet")

if (!file.exists(cache_file)) {
  message("Cache not found. Querying Athena and saving to cache...")

  conn <- dbConnect(noctua::athena())

  assessment_card <- dbGetQuery(
    conn,
    "SELECT * FROM pinval.vw_assessment_card"
  )

  write_parquet(assessment_card, cache_file)

} else {
  message("Reading cached file")
  changes <- read_parquet(cache_file)
}


```

```{r}
tz_local      <- "America/Chicago"
baseline_date <- as.Date("2025-09-04")

log_group_name  <- "/aws-cloudfront/pinval-prod"
log_stream_name <- "CloudFront_E3AIN3GTEPCCYR"

out_dir <- fs::path_abs("data/cloudwatch_cache")
fs::dir_create(out_dir)
message("Writing to: ", out_dir)

cloudwatchlogs_client <- paws::cloudwatchlogs()

to_epoch_ms_utc <- function(x_dt_local) {
  as.numeric(with_tz(x_dt_local, "UTC")) * 1000
}


fetch_events_for_day <- function(date_local) {
  start_local <- as.POSIXct(date_local, tz = tz_local)
  end_local   <- start_local + days(1)

  start_ms <- to_epoch_ms_utc(start_local)
  end_ms   <- to_epoch_ms_utc(end_local)

  # First call
  resp <- cloudwatchlogs_client$get_log_events(
    logGroupName  = log_group_name,
    logStreamName = log_stream_name,
    startTime     = start_ms,
    endTime       = end_ms,
    startFromHead = TRUE
  )

  events <- resp$events %||% list()
  prev_token <- NULL
  next_token <- resp$nextForwardToken %||% NULL

  # Keep calling until token stops changing
  while (!is.null(next_token) && !identical(next_token, prev_token)) {
    prev_token <- next_token
    resp <- cloudwatchlogs_client$get_log_events(
      logGroupName      = log_group_name,
      logStreamName     = log_stream_name,
      nextToken         = next_token,
      startFromHead     = TRUE
    )
    events <- c(events, resp$events %||% list())
    next_token <- resp$nextForwardToken %||% NULL
  }

  if (length(events) == 0) {
    return(tibble(
      timestamp = numeric(0),
      message   = character(0),
      datetime  = as.POSIXct(character(0)),
      date      = as.Date(character(0))
    ))
  }

  tibble(
    timestamp = map_dbl(events, ~ .x$timestamp),
    message   = map_chr(events, ~ .x$message)
  ) %>%
    mutate(
      # CloudWatch timestamps are epoch ms in UTC
      datetime = with_tz(as_datetime(timestamp / 1000, tz = "UTC"), tz_local),
      date     = as.Date(datetime)
    ) %>%
    # keep only rows that actually fall on this local date (defensive)
    filter(date == date_local)
}

# Today in local time
today_local <- as.Date(with_tz(Sys.time(), tz_local))

# Look for already-cached files like logs_YYYY-MM-DD.parquet
cached_files <- dir_ls(out_dir, regexp = "logs_\\d{4}-\\d{2}-\\d{2}\\.parquet$", type = "file")
cached_dates <- str_match(basename(cached_files), "logs_(\\d{4}-\\d{2}-\\d{2})\\.parquet")[,2]
cached_dates <- as.Date(na.omit(cached_dates))

# Build the full sequence of dates you care about
all_dates <- seq(baseline_date, today_local, by = "day")

# Download plan:
# - Always refresh today's file
# - For other dates, only download if not cached yet
dates_to_fetch <- c(
  setdiff(all_dates[all_dates != today_local], cached_dates),
  today_local
) %>% sort()

message("Dates to fetch (", length(dates_to_fetch), "): ",
        paste(format(dates_to_fetch), collapse = ", "))

# ------------------------------
# DOWNLOAD + CACHE
# ------------------------------
walk(dates_to_fetch, function(d) {
  out_file <- fs::path(out_dir, paste0("logs_", format(d), ".parquet"))
  message("Target file: ", fs::path_abs(out_file))

  if (fs::file_exists(out_file) && d != today_local) {
    message("Skipping ", format(d), " (already cached).")
    return(invisible(NULL))
  }

  message("Fetching ", format(d), " ...")
  df <- tryCatch(fetch_events_for_day(d), error = function(e) {
    warning("Failed to fetch ", format(d), ": ", conditionMessage(e))
    tibble::tibble()
  })

  arrow::write_parquet(df, out_file)
  message("Wrote ", nrow(df), " events to ", fs::path_abs(out_file))
  stopifnot(fs::file_exists(out_file))
  message("Verified exists: ", as.character(fs::file_size(out_file)), " bytes")
})

logs <- fs::dir_ls(
  out_dir,
  regexp = "logs_\\d{4}-\\d{2}-\\d{2}\\.parquet$",
  type   = "file"
) %>%
  purrr::map_dfr(arrow::read_parquet)

```


```{r}
fields <- c(
  "date","time","sc-bytes","c-ip","cs-method","cs(Host)",
  "cs-uri-stem","sc-status","cs(Referer)","cs(User-Agent)",
  "cs-uri-query","x-host-header","cs-protocol","cs-bytes",
  "time-taken","x-forwarded-for","x-edge-response-result-type",
  "cs-protocol-version","c-port","time-to-first-byte",
  "sc-content-type","sc-content-len"
)

logs_expanded <- logs %>%
  mutate(parsed = map(message, ~ {
    val <- tryCatch(jsonlite::fromJSON(.x, simplifyVector = TRUE), error = function(e) NULL)
    if (is.null(val)) return(rep(NA_character_, length(fields)))
    out <- setNames(vector("list", length(fields)), fields)
    for (nm in fields) {
      out[[nm]] <- if (nm %in% names(val)) as.character(val[[nm]]) else NA_character_
    }
    out
  })) %>%
  # Remove any existing columns that would clash with parsed names
  select(-any_of(fields)) %>%
  tidyr::unnest_wider(parsed, names_repair = "minimal")

# Optional coercions
logs_expanded <- logs_expanded %>%
  mutate(
    `sc-status`          = suppressWarnings(as.integer(`sc-status`)),
    `sc-bytes`           = suppressWarnings(as.integer(`sc-bytes`)),
    `cs-bytes`           = suppressWarnings(as.integer(`cs-bytes`)),
    `c-port`             = suppressWarnings(as.integer(`c-port`)),
    `time-taken`         = suppressWarnings(as.numeric(`time-taken`)),
    `time-to-first-byte` = suppressWarnings(as.numeric(`time-to-first-byte`))
  )


```

## Request count by class

```{r}

```
